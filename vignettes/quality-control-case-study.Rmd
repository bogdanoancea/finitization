---
title: "Quality Control with Bounded Sampling"
author: "Bogdan Oancea"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Quality Control with Bounded Sampling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)
```

## Overview

This vignette demonstrates the practical importance of finitization for quality control 
applications with bounded sampling constraints. We show how incorrect distributional 
modeling can lead to substantial operational errors and statistical inference problems.

## Problem Description

A manufacturing facility produces electronic components in batches of N = 20 units. 
Due to destructive testing constraints, quality inspectors can test at most n = 3 
components per batch. Each component has a defect probability of p = 0.05 (5%). 
Batches are rejected if more than d = 1 defective unit is found in the sample.

**Key question**: How should we model the bounded defect count distribution?

### Three Approaches

1. **Finitization**: Preserves the first $n$ moments of the parent distribution
2. **Truncation**: Conditions on $X \le n$ and renormalizes
3. **Capping**: Simple truncation at the boundary

```{r libraries}
library(finitization)

if (!requireNamespace("truncdist", quietly = TRUE)) {
  stop("Package 'truncdist' is required for this vignette.")
}

if (!requireNamespace("ggplot2", quietly = TRUE)) {
  stop("Package 'ggplot2' is required for this vignette.")
}
```
```{r parameters}
# System parameters
N <- 20          # Batch size
p <- 0.05        # Defect probability
n <- 3           # Maximum sample size (finitization order)
d <- 1           # Rejection threshold
n_batches <- 1e6 # Number of batches to simulate

# Calculate tail probability
tail_prob <- stats::pbinom(n, size = N, prob = p, lower.tail = FALSE)
cat("P(X >", n, ") =", round(tail_prob, 4), 
    sprintf("(%.2f%%)", 100 * tail_prob), "\n")
```

With approximately 1.6% of batches naturally producing more than 3 defects, the 
capacity constraint is meaningful but not extreme.

## Simulation Study

We simulate 1,000,000 quality control decisions to compare the three approaches.
```{r simulation}
# Set seed for reproducibility
set.seed(789)

# Generate defect counts using different methods
defects_fin <- finitization::rbinom(n = n, p = p, N = N, no = n_batches)
defects_trunc <- truncdist::rtrunc(n = n_batches, spec = "binom", 
                        a = -1, b = n, size = N, prob = p)
defects_capped <- pmin(stats::rbinom(n_batches, size = N, prob = p), n)
```

### Rejection Rates
```{r rejection_rates}
# Calculate rejection rates
rejection_fin <- mean(defects_fin > d)
rejection_trunc <- mean(defects_trunc > d)
rejection_capped <- mean(defects_capped > d)

# Theoretical unbounded rejection rate
theoretical_rejection <- stats::pbinom(d, size = N, prob = p, lower.tail = FALSE)

# Create results data frame
rejection_results <- data.frame(
  Method = c("Theoretical (unbounded)", "Finitized", "Truncated", "Capped"),
  Rejection_Rate = c(theoretical_rejection, rejection_fin, 
                     rejection_trunc, rejection_capped),
  Rejections = c(NA, 
                 rejection_fin * n_batches, 
                 rejection_trunc * n_batches, 
                 rejection_capped * n_batches)
)

rejection_results$vs_Finitized <- with(rejection_results, 
  ifelse(is.na(Rejections), NA,
         sprintf("%+.1f%%", 100 * (Rejection_Rate / rejection_fin - 1)))
)

knitr::kable(rejection_results, 
             digits = 4,
             col.names = c("Method", "Rejection Rate", "Rejections (millions)", "vs. Finitized"),
             caption = "Batch rejection rates for different modeling approaches")
```

**Key observation**: Truncation and capping produce substantially higher rejection 
rates—approximately 33-39% more rejections than the finitized approach.

### Aggregate Statistics
```{r aggregate_stats}
# Calculate aggregate statistics
theoretical_mean <- N * p
theoretical_var <- N * p * (1 - p)

agg_stats <- data.frame(
  Method = c("Theoretical", "Finitized", "Truncated", "Capped"),
  Mean = c(theoretical_mean, 
           mean(defects_fin), 
           mean(defects_trunc), 
           mean(defects_capped)),
  Variance = c(theoretical_var, 
               var(defects_fin), 
               var(defects_trunc), 
               var(defects_capped))
)

agg_stats$Mean_Error <- with(agg_stats,
  sprintf("%.1f%%", 100 * (Mean - theoretical_mean) / theoretical_mean)
)

agg_stats$Var_Error <- with(agg_stats,
  sprintf("%.1f%%", 100 * (Variance - theoretical_var) / theoretical_var)
)

knitr::kable(agg_stats, 
             digits = 3,
             col.names = c("Method", "Mean", "Variance", "Mean Error", "Var Error"),
             caption = "Aggregate defect statistics from simulation")
```

```{r aggregate_bias_summary}
# Extract truncation and capping rows
bias_subset <- agg_stats[agg_stats$Method %in% c("Truncated", "Capped"), ]

# Numeric bias values
mean_bias_vals <- 100 * (bias_subset$Mean - theoretical_mean) / theoretical_mean
var_bias_vals  <- 100 * (bias_subset$Variance - theoretical_var) / theoretical_var

# Ranges (absolute values for interpretability)
mean_bias_range <- range(abs(mean_bias_vals))
var_bias_range  <- range(abs(var_bias_vals))

```
```markdown
Even within the simulation context, truncation and capping show clear biases in both
the mean (absolute error between `r sprintf("%.1f%%", mean_bias_range[1])` and `r sprintf("%.1f%%", mean_bias_range[2])`) and the variance (absolute error between `r sprintf("%.1f%%", var_bias_range[1])` and
`r sprintf("%.1f%%", var_bias_range[2])`).
```

## Moment Preservation Analysis

To understand the distributional properties directly, we generate 1,000,000 independent 
samples from each bounded distribution.
```{r moment_analysis}
# Generate large samples for moment analysis
n_large <- 1e6
set.seed(123)

sample_fin <- finitization::rbinom(n = n, p = p, N = N, no = n_large)
sample_trunc <- truncdist::rtrunc(n = n_large, spec = "binom", 
                       a = -1, b = n, size = N, prob = p)
sample_capped <- pmin(stats::rbinom(n_large, size = N, prob = p), n)

# Calculate moments
moment_results <- data.frame(
  Method = c("Theoretical", "Finitized", "Truncated", "Capped"),
  Mean = c(theoretical_mean,
           mean(sample_fin),
           mean(sample_trunc),
           mean(sample_capped)),
  Variance = c(theoretical_var,
               var(sample_fin),
               var(sample_trunc),
               var(sample_capped))
)

# Numeric biases (keep numbers!)
moment_results$Mean_Bias_num <- with(moment_results,
  ifelse(Method == "Theoretical", NA,
         100 * (Mean - theoretical_mean) / theoretical_mean)
)

moment_results$Var_Error_num <- with(moment_results,
  ifelse(Method == "Theoretical", NA,
         100 * (Variance - theoretical_var) / theoretical_var)
)

# Character versions for the table
moment_results$Mean_Bias <- ifelse(
  is.na(moment_results$Mean_Bias_num),
  NA,
  sprintf("%.2f%%", moment_results$Mean_Bias_num)
)

moment_results$Var_Error <- ifelse(
  is.na(moment_results$Var_Error_num),
  NA,
  sprintf("%.2f%%", moment_results$Var_Error_num)
)


knitr::kable(
  moment_results[, c("Method", "Mean", "Variance", "Mean_Bias", "Var_Error")],
  digits = 4,
  col.names = c("Method", "Mean", "Variance", "Mean Bias (%)", "Variance Error (%)"),
  caption = "Moment preservation from 1,000,000 independent samples"
)
```

```{r moment_bias_summary}
# Extract numeric biases
bias_fin_mean  <- abs(moment_results$Mean_Bias_num[moment_results$Method == "Finitized"])
bias_tr_mean   <- abs(moment_results$Mean_Bias_num[moment_results$Method == "Truncated"])
bias_cap_mean  <- abs(moment_results$Mean_Bias_num[moment_results$Method == "Capped"])

bias_fin_var   <- abs(moment_results$Var_Error_num[moment_results$Method == "Finitized"])
bias_tr_var    <- abs(moment_results$Var_Error_num[moment_results$Method == "Truncated"])
bias_cap_var   <- abs(moment_results$Var_Error_num[moment_results$Method == "Capped"])

# Ranges
mean_bias_range_trcap <- range(c(bias_tr_mean, bias_cap_mean))
var_bias_range_trcap  <- range(c(bias_tr_var,  bias_cap_var))

# Improvement factors (worst case vs finitized)
mean_improvement_range <- range(
  c(bias_tr_mean, bias_cap_mean) / bias_fin_mean
)
var_improvement_range <- range(
  c(bias_tr_var, bias_cap_var) / bias_fin_var
)

mean_bias_range_trcap
var_bias_range_trcap
mean_improvement_range
var_improvement_range
```

```markdown
**Main findings**:

- **Finitization**: Near-perfect moment preservation (absolute mean error `r sprintf("%.2f%%", bias_fin_mean)` 
and absolute variance error `r sprintf("%.2f%%", bias_fin_var)`).

- **Truncation and capping**: Substantial distortion remains even with bounded support.
  Mean bias ranges from `r sprintf("%.2f%%", mean_bias_range_trcap[1])` to `r sprintf("%.2f%%", mean_bias_range_trcap[2])`, while variance error ranges from `r sprintf("%.2f%%", var_bias_range_trcap[1])` to `r sprintf("%.2f%%", var_bias_range_trcap[2])`.

Overall, finitization improves mean accuracy by a factor between `r sprintf("%.0f×", mean_improvement_range[1])` and
`r sprintf("%.0f×", mean_improvement_range[2])`, and variance accuracy by a factor between `r sprintf("%.0f×", var_improvement_range[1])` and `r sprintf("%.0f×", var_improvement_range[2])` relative to truncation and capping.
```
This represents a $\approx$ **25-400 fold improvement** in moment accuracy for finitization.

## Visualizing the Differences

### PMF Comparison
```{r pmf_comparison, fig.cap="Probability mass function comparison showing how truncation and capping distort the distribution"}
# Calculate empirical PMFs
k <- 0:n
pmf_data <- data.frame(
  k = rep(k, 4),
  Probability = c(
    stats::dbinom(k, size = N, prob = p),  # Theoretical
    sapply(k, function(x) mean(sample_fin == x)),
    sapply(k, function(x) mean(sample_trunc == x)),
    sapply(k, function(x) mean(sample_capped == x))
  ),
  Method = factor(rep(c("Theoretical", "Finitized", "Truncated", "Capped"), 
                      each = length(k)),
                  levels = c("Theoretical", "Finitized", "Truncated", "Capped"))
)

# Plot
ggplot2::ggplot(pmf_data, ggplot2::aes(x = k, y = Probability, fill = Method)) +
ggplot2::geom_col(position = ggplot2::position_dodge(width = 0.8), alpha = 0.7) +
ggplot2::geom_vline(xintercept = d + 0.5, linetype = "dashed", color = "red") +
ggplot2::scale_fill_manual(values = c("Theoretical" = "#E63946",
                           "Finitized" = "#2E86AB", 
                           "Truncated" = "#A23B72",
                           "Capped" = "#F18F01")) +
ggplot2::labs(
title = "Probability Mass Function Comparison",
subtitle = sprintf("Binomial(N=%d, p=%.2f) bounded at n=%d", N, p, n),
x = "Number of Defects",
y = "Probability",
fill = "Method"
) +
ggplot2::annotate("text", x = d + 0.5, y = max(pmf_data$Probability) * 0.9,
       label = "Rejection\nthreshold", hjust = -0.1, color = "red", size = 3) +
ggplot2::theme_minimal() +
ggplot2::theme(legend.position = "top")

```

### Moment Errors by Method
```{r moment_errors, fig.cap="Moment estimation errors showing finitization's superior accuracy"}
error_data <- data.frame(
  Method = rep(c("Finitized", "Truncated", "Capped"), 2),
  Moment = factor(rep(c("Mean", "Variance"), each = 3),
                  levels = c("Mean", "Variance")),
  Error = c(
    100 * (mean(sample_fin) - theoretical_mean) / theoretical_mean,
    100 * (mean(sample_trunc) - theoretical_mean) / theoretical_mean,
    100 * (mean(sample_capped) - theoretical_mean) / theoretical_mean,
    100 * (var(sample_fin) - theoretical_var) / theoretical_var,
    100 * (var(sample_trunc) - theoretical_var) / theoretical_var,
    100 * (var(sample_capped) - theoretical_var) / theoretical_var
  )
)


ggplot2::ggplot(error_data, ggplot2::aes(x = Method, y = Error, fill = Method)) +
ggplot2::geom_col(alpha = 0.7) +
ggplot2::geom_hline(yintercept = 0, linetype = "solid") +
ggplot2::geom_text(ggplot2::aes(label = sprintf("%.2f%%", Error)), 
        vjust = ifelse(error_data$Error >= 0, -0.5, 1.5),
        size = 3.5) +
ggplot2::facet_wrap(~ Moment, scales = "free_y") +
ggplot2::scale_fill_manual(values = c("Finitized" = "#2E86AB", 
                           "Truncated" = "#A23B72",
                           "Capped" = "#F18F01")) +
ggplot2::labs(
title = "Moment Estimation Errors",
subtitle = "Percentage deviation from theoretical Binomial(20, 0.05) moments",
x = "Method",
y = "Percentage Error"
) +
ggplot2::theme_minimal() +
ggplot2::theme(
legend.position = "none",
strip.background = ggplot2::element_rect(fill = "gray90", color = NA),
strip.text = ggplot2::element_text(face = "bold")
)


```

## Financial Impact

Assuming a cost of $100 per falsely rejected batch:
```{r financial_impact}
cost_per_rejection <- 100

financial_data <- data.frame(
  Method = c("Finitized", "Truncated", "Capped"),
  Rejections = c(rejection_fin * n_batches,
                 rejection_trunc * n_batches,
                 rejection_capped * n_batches)
)

financial_data$Cost <- financial_data$Rejections * cost_per_rejection
financial_data$Excess_Rejections <- financial_data$Rejections - 
                                    financial_data$Rejections[1]
financial_data$Excess_Cost <- financial_data$Cost - financial_data$Cost[1]

knitr::kable(financial_data,
             digits = 0,
             col.names = c("Method", "Rejections", "Cost (USD)", "Excess Rejections", "Excess Cost (USD)"),
             caption = "Financial impact per 1,000,000 batches",
             format.args = list(big.mark = ","))
```

**Key finding**: Using truncation or capping instead of finitization costs an important additional cost due to unnecessary rejections.

## Implications for Statistical Applications

The moment distortions have serious consequences for statistical inference procedures.

### Statistical Process Control

Control chart limits depend on accurate variance estimates:
```{r spc_impact}
# Calculate SPC implications
sigma_true   <- sqrt(theoretical_var)      # "correct" sigma (true process sigma)
sigma_trunc  <- sqrt(var(sample_trunc))    # estimated sigma under truncation
sigma_capped <- sqrt(var(sample_capped))   # estimated sigma under capping

# Width ratios (estimated / true)
width_ratio_trunc  <- sigma_trunc  / sigma_true
width_ratio_capped <- sigma_capped / sigma_true

# False alarm rate under true sigma, using limits based on sigma_est:
# P(|Z| > 3 * (sigma_est / sigma_true)) = 2 * (1 - Phi(3 * width_ratio))
fa_correct <- 2 * (1 - pnorm(3 * 1.0))
fa_trunc   <- 2 * (1 - pnorm(3 * width_ratio_trunc))
fa_capped  <- 2 * (1 - pnorm(3 * width_ratio_capped))

fa_increase_trunc  <- 100 * (fa_trunc  / fa_correct - 1)
fa_increase_capped <- 100 * (fa_capped / fa_correct - 1)

spc_results <- data.frame(
  Method = c("Correct (Finitized)", "Truncated", "Capped"),
  Sigma = c(sigma_true, sigma_trunc, sigma_capped),
  Width_Ratio = c(1.0, width_ratio_trunc, width_ratio_capped),
  False_Alarm_Increase_pct = c(0, fa_increase_trunc, fa_increase_capped)
)

knitr::kable(
  spc_results,
  digits = 3,
  col.names = c("Method", "σ estimate", "Limit width ratio", "False-alarm increase (%)"),
  caption = "Impact on statistical process control under distorted variance (Normal approximation)"
)
```

```markdown
Control limits based on the truncated model are `r sprintf("%.1f%%", 100 * (1 - width_ratio_trunc))` narrower than the correct limits, which increases the false-alarm rate by approximately  **`r sprintf("%.0f%%", fa_increase_trunc)`**. 
Under the capped model, limits are  `r sprintf("%.1f%%", 100 * (1 - width_ratio_capped))` narrower, corresponding to a false-alarm increase of about  **`r sprintf("%.0f%%", fa_increase_capped)`**.
```

### Confidence Intervals
```{r confidence_intervals}
# Nominal level
alpha <- 0.05
z <- qnorm(1 - alpha / 2)

# Variance estimates
var_fin   <- var(sample_fin)
var_trunc <- var(sample_trunc)
var_cap   <- var(sample_capped)

# CI width ratios relative to correct (finitized)
width_ratio_trunc <- sqrt(var_trunc / var_fin)
width_ratio_cap   <- sqrt(var_cap   / var_fin)

# Actual coverage assuming Normal approximation:
# Coverage = 2 * Phi(z / width_ratio) - 1
cov_fin   <- 2 * pnorm(z) - 1
cov_trunc <- 2 * pnorm(z * width_ratio_trunc) - 1
cov_cap   <- 2 * pnorm(z * width_ratio_cap)   - 1

ci_results <- data.frame(
  Method = c("Correct (Finitized)", "Truncated", "Capped"),
  Variance = c(var_fin, var_trunc, var_cap),
  Width_Ratio = c(1.0, width_ratio_trunc, width_ratio_cap),
  Actual_Coverage = c(cov_fin, cov_trunc, cov_cap)
)

knitr::kable(
  ci_results,
  digits = 3,
  col.names = c("Method", "Variance estimate", "CI width ratio", "Actual coverage"),
  caption = "Impact of variance distortion on confidence interval coverage"
)
```

```markdown
Under truncation, confidence intervals are  **`r sprintf("%.1f%%", 100 * (1 - width_ratio_trunc))`** narrower than nominal,  resulting in an actual coverage of approximately  **`r sprintf("%.1f%%", 100 * cov_trunc)`** instead of the nominal 95%.  
Under capping, intervals are **`r sprintf("%.1f%%", 100 * (1 - width_ratio_cap))`** narrower, with coverage reduced to about  **`r sprintf("%.1f%%", 100 * cov_cap)`**.
```

### Process Capability
```{r capability}
# Recompute sigmas from samples (robust to chunk order)
sigma_fin   <- sqrt(var(sample_fin))
sigma_trunc <- sqrt(var(sample_trunc))
sigma_cap   <- sqrt(var(sample_capped))

# Cp ratios relative to correct (finitized)
cp_ratio_trunc <- sigma_fin / sigma_trunc
cp_ratio_cap   <- sigma_fin / sigma_cap

cp_results <- data.frame(
  Method = c("Correct (Finitized)", "Truncated", "Capped"),
  Sigma_Estimate = c(sigma_fin, sigma_trunc, sigma_cap),
  Cp_Ratio = c(1.0, cp_ratio_trunc, cp_ratio_cap),
  Overestimation_pct = c(
    NA,
    100 * (cp_ratio_trunc - 1),
    100 * (cp_ratio_cap   - 1)
  )
)

# Character column for display only
cp_results$Effect <- ifelse(
  is.na(cp_results$Overestimation_pct),
  "True capability",
  sprintf("Overestimates by %.1f%%", cp_results$Overestimation_pct)
)

knitr::kable(
  cp_results[, c("Method", "Sigma_Estimate", "Cp_Ratio", "Effect")],
  digits = 3,
  col.names = c("Method", "σ Estimate", "Cp Ratio", "Effect"),
  caption = "Impact of variance distortion on process capability indices (Cp)"
)
```


```markdown
A process with true Cp = 1.00 (minimally capable) would appear to have Cp ≈ **`r sprintf("%.2f", cp_ratio_trunc)`** under truncation   and Cp ≈ **`r sprintf("%.2f", cp_ratio_cap)`** under capping.  
This corresponds to an overestimation of capability by **`r sprintf("%.1f%%", 100 * (cp_ratio_trunc - 1))`** and   **`r sprintf("%.1f%%", 100 * (cp_ratio_cap - 1))`**, respectively, potentially masking quality issues that would otherwise require corrective action.
```

## Summary and Recommendations

This case study demonstrates that for bounded count data with tail probabilities around 
1-2%, the choice of bounded distribution model has critical consequences:

### Moment Preservation (The Unambiguous Criterion)
```{r summary_table}
# Extract numeric errors (absolute values, excluding theoretical row)
err <- subset(
  moment_results,
  Method %in% c("Finitized", "Truncated", "Capped"),
  select = c(Method, Mean_Bias_num, Var_Error_num)
)

err$Mean_Error_abs <- abs(err$Mean_Bias_num)
err$Var_Error_abs  <- abs(err$Var_Error_num)

# Baseline (finitized)
mean_base <- err$Mean_Error_abs[err$Method == "Finitized"]
var_base  <- err$Var_Error_abs[err$Method == "Finitized"]

# Improvement factors vs finitized
err$Mean_Improvement <- err$Mean_Error_abs / mean_base
err$Var_Improvement  <- err$Var_Error_abs  / var_base

# Build presentation table
summary_data <- data.frame(
  Method = err$Method,
  Mean_Error = sprintf("%.2f%%", err$Mean_Error_abs),
  Variance_Error = sprintf("%.2f%%", err$Var_Error_abs),
  `vs Finitized` = ifelse(
    err$Method == "Finitized",
    "Baseline",
    sprintf(
      "%.0fx (mean), %.0fx (variance) worse",
      err$Mean_Improvement,
      err$Var_Improvement
    )
  )
)

knitr::kable(
  summary_data,
  align = "lccc",
  caption = "Summary of moment preservation accuracy (computed from simulation)"
)
```

### Checking MFPS

Always verify parameters are within the Maximum Feasible Parameter Space:
```{r mfps_check}
# Check MFPS for our example
mfps <- getBinomialMFPS(n = n, N = N)
cat("MFPS for n =", n, ", N =", N, ":", mfps, "\n")
cat("Our p =", p, "is within MFPS:", p >= mfps[1] && p <= mfps[2], "\n")
```

## References

- Golnabi, S., Levy, M. S., & Cochran, J. J. (2009). Finitizing power series distributions. 
*Statistics & Probability Letters*, 79(17), 1829-1832.

- Levy, M. S., Cochran, J. J., & Golnabi, S. (2012). A moment preserving finitization 
across the power series family of probability distributions. *Communications in Statistics 
– Theory and Methods*, 41(4), 653-664.

- Levy, M. S., Cochran, J. J., Golnabi, S., & Kirtland, J. A. (2019). Application of 
finitized power series distributions to accelerate variate generation. Part I: two useful 
algorithms. *International Transactions in Operational Research*, 26(6), 2305-2323.

## Session Info
```{r session_info}
sessionInfo()
```
